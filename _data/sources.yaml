- id: doi:10.1109/JBHI.2025.3562090
  description: |
    Federated learning (FL) has gained prominence in EEG-based emotion recognition due to its ability to enable secure collaborative training without centralized data. 
    However, traditional FL faces challenges due to model and data heterogeneity in smart healthcare settings. 
    For example, medical institutions have varying computational resources, which creates a need for personalized local models. 
    Moreover, EEG data from medical institutions typically face data heterogeneity issues stemming from limitations in participant availability, ethical constraints, and cultural differences among subjects, which can slow model convergence and degrade model performance. 
    To address these challenges, we propose FedKDC, a novel FL framework that incorporates clustered knowledge distillation (CKD). 
    This method introduces a consensus-based distributed learning mechanism to facilitate the clustering process. 
    It then enhances the convergence speed through intraclass distillation and reduces the negative impact of heterogeneity through interclass distillation. 
    Additionally, we introduce a DriftGuard mechanism to mitigate client drift, along with an entropy reducer to decrease the entropy of aggregated knowledge. 
    The framework is validated on the SEED, SEED-IV, SEED-FRA, and SEED-GER datasets, demonstrating its effectiveness in scenarios where both the data and the models are heterogeneous. 
    Experimental results show that FedKDC outperforms other FL frameworks in emotion recognition, achieving a maximum average accuracy of 85.2%, and in convergence efficiency, with faster and more stable convergence. 
    Our code is made publicly available at: https://github.com/wdqdp/FedKDC.
  image: images/JBHI_2025_FedKDC.jpg
  date: 2025-04-16
